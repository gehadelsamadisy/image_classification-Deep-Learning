# CIFAR-10 Image Classification with Convolutional Neural Networks

A deep learning project implementing and comparing CNN architectures for image classification on the CIFAR-10 dataset.

## ğŸ“‹ Overview

This project demonstrates the development and comparison of two CNN models:

- **Baseline Model**: A simple 2-layer CNN for establishing performance benchmarks
- **Improved Model**: An enhanced architecture with batch normalization, dropout, and deeper convolutional blocks

## ğŸ¯ Results

| Model           | Test Accuracy |
| --------------- | ------------- |
| Baseline CNN    | 67.66%        |
| Improved CNN    | **83.65%**    |
| **Improvement** | +15.99%       |

## ğŸ“Š Dataset

**CIFAR-10** consists of 60,000 32Ã—32 color images across 10 classes:

| Class      | Examples |
| ---------- | -------- |
| Airplane   | âœˆï¸       |
| Automobile | ğŸš—       |
| Bird       | ğŸ¦       |
| Cat        | ğŸ±       |
| Deer       | ğŸ¦Œ       |
| Dog        | ğŸ•       |
| Frog       | ğŸ¸       |
| Horse      | ğŸ´       |
| Ship       | ğŸš¢       |
| Truck      | ğŸšš       |

**Split:**

- Training: 32,000 images
- Validation: 8,000 images
- Test: 10,000 images

## ğŸ—ï¸ Model Architectures

### Baseline Model

```
Conv2D(32) â†’ MaxPool â†’ Conv2D(64) â†’ MaxPool â†’ Flatten â†’ Dense(64) â†’ Dense(10)
```

### Improved Model

```
[Conv Block 1] â†’ [Conv Block 2] â†’ [Conv Block 3] â†’ Dense Layers â†’ Output

Each Conv Block:
â”œâ”€â”€ Conv2D + BatchNorm + ReLU
â”œâ”€â”€ Conv2D + BatchNorm + ReLU
â”œâ”€â”€ MaxPooling2D
â””â”€â”€ Dropout(0.25)

Dense Layers:
â”œâ”€â”€ Dense(256) + BatchNorm + Dropout(0.5)
â”œâ”€â”€ Dense(128) + BatchNorm + Dropout(0.5)
â””â”€â”€ Dense(10, softmax)
```

## ğŸ› ï¸ Key Techniques

- **Batch Normalization**: Stabilizes training and allows higher learning rates
- **Dropout**: Prevents overfitting (0.25 after conv blocks, 0.5 after dense layers)
- **Padding='same'**: Preserves spatial dimensions through convolutions
- **Learning Rate Scheduling**: ReduceLROnPlateau reduces LR when validation loss plateaus
- **Early Stopping**: Stops training when validation loss stops improving

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install tensorflow numpy matplotlib scikit-learn
```

### Running the Notebook

1. Open `img-class.ipynb` in Jupyter Notebook or Google Colab
2. Run all cells sequentially
3. Training takes approximately 5-10 minutes on GPU

## ğŸ“ˆ Training Callbacks

| Callback          | Configuration                          |
| ----------------- | -------------------------------------- |
| ReduceLROnPlateau | factor=0.5, patience=3, min_lr=1e-6    |
| EarlyStopping     | patience=10, restore_best_weights=True |


```

## ğŸ” Visualizations

The notebook includes:

- Training/validation accuracy and loss curves
- Sample predictions with true vs predicted labels
- Color-coded results (green = correct, red = incorrect)

## ğŸ’¡ Key Findings

1. **Deeper architectures** with more convolutional blocks capture hierarchical features better
2. **Batch normalization** significantly speeds up training convergence
3. **Dropout regularization** effectively reduces overfitting
4. **Learning rate scheduling** helps fine-tune the model in later epochs

## ğŸ›¡ï¸ Requirements

- Python 3.8+
- TensorFlow 2.x
- NumPy
- Matplotlib
- scikit-learn

## ğŸ“ License

This project is for educational purposes.
